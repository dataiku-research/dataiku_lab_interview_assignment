{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Data Augmentation - Credit Card Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a ML assignment as a part of the recruitment process at the Dataiku Lab.\n",
    "\n",
    "The cells will walk you through the questions and parts of code to complete/comment. \n",
    "\n",
    "If a question is not marked as [OPTIONAL], this means it is mandatory to answer, and it might be necessary in order to run the subsequent cells of the notebook. [OPTIONAL] questions can be skipped. \n",
    "\n",
    "Answering the mandatory questions should take around 2 hours. Answering all questions should take around 4 hours.\n",
    "\n",
    "Pay attention to the `#TODO` tags and good work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = 'data/creditcard.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Class'\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(df.drop(target, axis=1), df[target], test_size=0.3, random_state=seed)\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "X_train = x_scaler.fit_transform(X_train_df)\n",
    "\n",
    "X_test = x_scaler.transform(X_test_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What could be said regarding the class balance? Compute the `difference_in_class_occurences` and the `class_occurences_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_class_occurences = #TODO\n",
    "class_occurences_ratio = #TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Synthesizer and Augment Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following you can find an implementation of the Tabular Variational Auto Encoder, a model used to synthesize tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "    \n",
    "\n",
    "class TVAE(nn.Module):\n",
    "    \"\"\" Tabular Variational Auto Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in:int, lin_layers:list, latent_dim:int=3):\n",
    "       \n",
    "        #Encoder\n",
    "        super(TVAE,self).__init__()\n",
    "        no = [D_in] + lin_layers\n",
    "        list_of_layers = [\n",
    "            nn.Sequential(nn.Linear(no[i], no[i+1]), nn.BatchNorm1d(no[i+1]), nn.ReLU())\n",
    "            for i in range(len(no)-1)\n",
    "            ]\n",
    "        self.encoder = nn.Sequential(*list_of_layers)\n",
    "        self.out_features_ = self.encoder[-1][0].out_features\n",
    "        \n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(self.out_features_, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, self.out_features_)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(self.out_features_)\n",
    "        \n",
    "        # Decoder\n",
    "        no =  lin_layers[::-1] + [D_in]\n",
    "        list_of_layers= [\n",
    "            nn.Sequential(nn.Linear(no[i], no[i+1]), nn.BatchNorm1d(no[i+1]), nn.ReLU())\n",
    "            for i in range(len(no)-1)\n",
    "            ]\n",
    "        # no ReLU in the last layer\n",
    "        list_of_layers[-1] = list_of_layers[-1][:-1]\n",
    "        self.decoder = nn.Sequential(*list_of_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(self.encoder(x))))\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = F.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = F.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        return self.decoder(fc4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL] Q2: What does the `reparametrize` function do during training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell implement the loss we need to minimize to train the TVAE. It's composed of two losses: the Mean Squared Error loss and the Kullback-Liebler Divergence loss. \n",
    "* Q3. Complete the implementation of the MSE loss\n",
    "* Q4. Explain what the MSE loss measures and why we want it to be low\n",
    "* [OPTIONAL] Q5. The expression `loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())` is the analytical expression of the KL divergence between two Gaussian distributions: $\\mathcal{N}(mu, e^{logvar})$ and $\\mathcal{N}(0, 1)$. Explain what the KLD loss measures and why we want it to be low. \n",
    "* [OPTIONAL] Q6. What could be an alternative implementation of `loss_KLD` using losses available in torch? In case you didn't know there was an analytical expression for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = # TODO\n",
    "    \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the training data for the TVAE containing only the fraud class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fraud = X_train[y_train==1]\n",
    "X_test_fraud = X_test[y_test==1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's instantiate the synthesizer model and train on fraud data. \n",
    "\n",
    "Q7. What is the input dimensions of the model? Complete below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "layers_in = [64, 32]\n",
    "latent_dim = 16\n",
    "\n",
    "model_vae = TVAE(D_in, layers_in, latent_dim=latent_dim).to(device)\n",
    "\n",
    "opt = optim.Adam(model_vae.parameters(), lr=0.01)\n",
    "\n",
    "loss_func = customLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop below and check the loss behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100 \n",
    "batch_size = 64\n",
    "\n",
    "x = torch.tensor(X_train_fraud.astype('float32'))\n",
    "x_test = torch.tensor(X_test_fraud.astype('float32'))\n",
    "\n",
    "training_loader = DataLoader(x, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "validation_loader = DataLoader(x_test, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "loss_by_epoch = []\n",
    "vloss_by_epoch = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    model_vae.train(True)\n",
    "    \n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, x_batch in enumerate(training_loader):\n",
    "        opt.zero_grad()\n",
    "        recon, mu, logvar = model_vae(x_batch)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_func(recon, x_batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        opt.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / (i + 1)\n",
    "    \n",
    "    if epoch % 20==0:\n",
    "        model_vae.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs = vdata\n",
    "            voutputs, vmu, vlogvar = model_vae(vinputs)\n",
    "            vloss = loss_func(voutputs, vinputs, vmu, vlogvar)\n",
    "            running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('Epoch {} - LOSS train {} valid {}'.format(epoch, avg_loss, avg_vloss))\n",
    "        model_vae.train(True)\n",
    "    else:\n",
    "        print('Epoch {} - LOSS train {}'.format(epoch, avg_loss))\n",
    "        \n",
    "    loss_by_epoch.append(avg_loss)\n",
    "    vloss_by_epoch.append(avg_vloss.cpu().detach().numpy())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_by_epoch, label='train loss')\n",
    "plt.plot(vloss_by_epoch, color='darkorange', label='val loss')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below uses the trained TVAE to generate synthetic data, sampling from a Gaussian distribution with specific mean and variance. The drawn samples are then decoded and mapped into the input space as synthetic new samples. \n",
    "* Q8. What are the mean and variance of the Gaussian distribution we sample from?\n",
    "* [OPTIONAL] Q9. What other kind of sampling we could do to generate new latent vectors and then new input samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model_vae, opt, training_loader, no_samples:int):\n",
    "    # get embeddings\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, xb in enumerate(training_loader):\n",
    "            opt.zero_grad()\n",
    "            _, mu_, logvar_ = model_vae(xb)\n",
    "            if batch_idx==0:\n",
    "                mu=mu_\n",
    "                logvar=logvar_\n",
    "            else:\n",
    "                mu=torch.cat((mu, mu_), dim=0)\n",
    "                logvar=torch.cat((logvar, logvar_), dim=0)\n",
    "\n",
    "    # sample from distribution defined by embeddings\n",
    "    sigma = torch.exp(logvar/2)\n",
    "    q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "    z = q.rsample(sample_shape=torch.Size([no_samples]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model_vae.decode(z).cpu().numpy()\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate new fraud samples, so to have a balanced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fake_fraud = generate_data(model_vae, opt, training_loader, no_samples=difference_in_class_occurences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the scatter plot the feature means of original and synthetic data.\n",
    "* [OPTIONAL] Q10. What should you expect to see in an ideal case where the synthetic data is actually from the very same distribution as real data?\n",
    "* [OPTIONAL] Q11. What could we do more to be sure the synthetic and real distributions are very similar ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train_fraud.mean(axis=0), X_fake_fraud.mean(axis=0))\n",
    "plt.xlabel('Features Means of Train data')\n",
    "plt.ylabel('Features Means of Synthetic data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Prediction Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12. Use any of the scikit-learn classifier and train a classifier on real data only to predict fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q13. Use scikit-learn metrics to check our model performance. \n",
    "\n",
    "Remember that our test set is from real data and that this is an imbalanced problem. In those cases accuracy is not the right metric to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Try to use the class weights. What did you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better calibrated model could also help your model to have performace more evenly distributed across classes. \n",
    "* [OPTIONAL] Q15. Why?\n",
    "* [OPTIONAL] Q16. Try a calibrated classifier from scikit-learn. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge real and synthetised data to train predictive models to classify fraud samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([199008, 199008]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_aug_vae = np.concatenate([X_train, X_fake_fraud])\n",
    "y_train_aug_vae = np.concatenate([y_train, np.ones((difference_in_class_occurences,))])\n",
    "\n",
    "np.unique(y_train_aug_vae, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17. Train again you first classifier, this time on the augmented train set and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q18. What could we conclude for this dataset about augmenting real data with TVAE sinthetically generated data?\n",
    "* [OPTIONAL] Q19. How can we make these conclusions more robust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the comparison of some tabular data synthetizers. Focus on the last two columns. \n",
    "\n",
    "The metric shown for classification problems is the F1-score, while the one used in regression problem is the R2 score. In both cases these scores represent the average performance of multiple ML models trained only on synthetised data and tested on real data. For instance for classification, the average performance of MLP, Logistic Regression, Adaboost and Decision Tree models is reported. \n",
    "\n",
    "The first row 'Identity' shows the performances for models trained only on real data instead.\n",
    "\n",
    "<center><img src=\"ctgan_results.png\" alt=\"ctgan results\" width=\"500\"/></center>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you agree with the following conclusions? Explain for each point your reasoning.\n",
    " \n",
    "* Q20. The TVAE is significantly better than the CTGAN for both classification and regression, thus I should use TVAE for tabular data augmentation. \n",
    "* Q21. Regardless of the ML model I want to use to predict on my data, TVAE would be a better synthethiser than CTGAN.\n",
    "* Q22. Both the TVAE and the CTGAN are anyway much worse than Identity, there's no way I could train on data generated from TVAE nor CTGAN, my performance wouldn't be good enough. There's no practical value in using these synthesizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [OPTIONAL] Q23. The TVAE used in this notebook can only work on numerical continuous variables. What could you do to make it work on categorical data? Any idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "ker_tda",
   "language": "python",
   "name": "ker_tda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
